{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14868513,"sourceType":"datasetVersion","datasetId":9511610}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport timm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom tqdm import tqdm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:12:59.936778Z","iopub.execute_input":"2026-02-17T17:12:59.937032Z","iopub.status.idle":"2026-02-17T17:13:05.123735Z","shell.execute_reply.started":"2026-02-17T17:12:59.936999Z","shell.execute_reply":"2026-02-17T17:13:05.122890Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\nprint(f\"GPU Memory Reserved:  {torch.cuda.memory_reserved()  / 1024**2:.2f} MB\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.126987Z","iopub.execute_input":"2026-02-17T17:13:05.127246Z","iopub.status.idle":"2026-02-17T17:13:05.156194Z","shell.execute_reply.started":"2026-02-17T17:13:05.127217Z","shell.execute_reply":"2026-02-17T17:13:05.155451Z"}},"outputs":[{"name":"stdout","text":"GPU Memory Allocated: 0.00 MB\nGPU Memory Reserved:  0.00 MB\nCUDA Available: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.158170Z","iopub.execute_input":"2026-02-17T17:13:05.158402Z","iopub.status.idle":"2026-02-17T17:13:05.172182Z","shell.execute_reply.started":"2026-02-17T17:13:05.158379Z","shell.execute_reply":"2026-02-17T17:13:05.171535Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.172868Z","iopub.execute_input":"2026-02-17T17:13:05.173056Z","iopub.status.idle":"2026-02-17T17:13:05.184926Z","shell.execute_reply.started":"2026-02-17T17:13:05.173037Z","shell.execute_reply":"2026-02-17T17:13:05.184248Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"Dataset paths\"\"\"\n\nTRAIN_REAL   = '/kaggle/input/datasets/lightvvcx/full-dataset/FULL_DATASET_FRAMES/train/real'\nTRAIN_ATTACK = '/kaggle/input/datasets/lightvvcx/full-dataset/FULL_DATASET_FRAMES/train/attack'\nTEST_REAL    = '/kaggle/input/datasets/lightvvcx/full-dataset/FULL_DATASET_FRAMES/test/real'\nTEST_ATTACK  = '/kaggle/input/datasets/lightvvcx/full-dataset/FULL_DATASET_FRAMES/test/attack'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.185647Z","iopub.execute_input":"2026-02-17T17:13:05.185842Z","iopub.status.idle":"2026-02-17T17:13:05.197170Z","shell.execute_reply.started":"2026-02-17T17:13:05.185824Z","shell.execute_reply":"2026-02-17T17:13:05.196634Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\"\"\"Dataset class\"\"\"\n\nclass AntispoofDataset(Dataset):\n    def __init__(self, real_path, attack_path, transform=None):\n        self.samples = []\n        self.transform = transform\n\n        for identity in os.listdir(real_path):\n            identity_path = os.path.join(real_path, identity)\n            if os.path.isdir(identity_path):\n                for img_name in os.listdir(identity_path):\n                    if img_name.endswith(('.jpg', '.png', '.jpeg')):\n                        self.samples.append((os.path.join(identity_path, img_name), 1))\n\n        for identity in os.listdir(attack_path):\n            identity_path = os.path.join(attack_path, identity)\n            if os.path.isdir(identity_path):\n                for img_name in os.listdir(identity_path):\n                    if img_name.endswith(('.jpg', '.png', '.jpeg')):\n                        self.samples.append((os.path.join(identity_path, img_name), 0))\n\n        real_count   = sum(1 for _, l in self.samples if l == 1)\n        attack_count = sum(1 for _, l in self.samples if l == 0)\n        print(f\"Loaded {len(self.samples)} samples - Real: {real_count} | Attack: {attack_count}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.198074Z","iopub.execute_input":"2026-02-17T17:13:05.198595Z","iopub.status.idle":"2026-02-17T17:13:05.210296Z","shell.execute_reply.started":"2026-02-17T17:13:05.198551Z","shell.execute_reply":"2026-02-17T17:13:05.209708Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"Data augmentation\"\"\"\n\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n    transforms.RandomGrayscale(p=0.05),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.3),\n    transforms.ToTensor(),\n    transforms.RandomApply([transforms.Lambda(lambda x: (x + 0.01 * torch.randn_like(x)).clamp(0, 1))], p=0.3),\n    transforms.Normalize([0.485,0.456,0.406],\n                         [0.229,0.224,0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],\n                         [0.229,0.224,0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.211105Z","iopub.execute_input":"2026-02-17T17:13:05.211390Z","iopub.status.idle":"2026-02-17T17:13:05.229065Z","shell.execute_reply.started":"2026-02-17T17:13:05.211359Z","shell.execute_reply":"2026-02-17T17:13:05.228343Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"Datasets and loaders\"\"\"\n\ntrain_dataset = AntispoofDataset(TRAIN_REAL, TRAIN_ATTACK, train_transform)\ntest_dataset  = AntispoofDataset(TEST_REAL,  TEST_ATTACK,  test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64,\n                          shuffle=True, num_workers=2, pin_memory=True)\n\ntest_loader  = DataLoader(test_dataset,  batch_size=64,\n                          shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.229859Z","iopub.execute_input":"2026-02-17T17:13:05.230080Z","iopub.status.idle":"2026-02-17T17:13:05.442948Z","shell.execute_reply.started":"2026-02-17T17:13:05.230058Z","shell.execute_reply":"2026-02-17T17:13:05.442258Z"}},"outputs":[{"name":"stdout","text":"Loaded 2806 samples - Real: 1333 | Attack: 1473\nLoaded 1302 samples - Real: 637 | Attack: 665\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## **ENHANCED MODEL with TEXTURE**","metadata":{}},{"cell_type":"code","source":"# ── NEW: DepthwiseBlock definition ────────────────────────────────────────────\nclass DepthwiseBlock(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(ch, ch, 3, padding=1, groups=ch, bias=False),  # depthwise\n            nn.Conv2d(ch, ch, 1, bias=False),                         # pointwise\n            nn.BatchNorm2d(ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.443847Z","iopub.execute_input":"2026-02-17T17:13:05.444129Z","iopub.status.idle":"2026-02-17T17:13:05.448823Z","shell.execute_reply.started":"2026-02-17T17:13:05.444095Z","shell.execute_reply":"2026-02-17T17:13:05.448005Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\"\"\"CNN - Texture Branch V2 + DepthwiseBlock\"\"\"\n\nclass MobileNetTexture(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        backbone = timm.create_model(\n            'mobilenetv3_small_100',\n            pretrained=True,\n            features_only=True\n        )\n\n        self.backbone = backbone\n\n        feature_info   = backbone.feature_info\n        mid_channels   = feature_info[2]['num_chs']\n        final_channels = feature_info[-1]['num_chs']\n\n        self.texture_branch = nn.Sequential(\n            nn.Conv2d(mid_channels, 64, 3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            # ── NEW: one depthwise block, lighter than ResBlock ──\n            DepthwiseBlock(64),\n            # ────────────────────────────────────────────────────\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten()\n        )\n\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(final_channels + 64, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n\n        mid_feat   = features[2]\n        final_feat = features[-1]\n\n        texture_feat = self.texture_branch(mid_feat)\n\n        main_feat = self.global_pool(final_feat)\n        main_feat = main_feat.view(main_feat.size(0), -1)\n\n        combined = torch.cat([main_feat, texture_feat], dim=1)\n\n        out = self.classifier(combined)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.449781Z","iopub.execute_input":"2026-02-17T17:13:05.450072Z","iopub.status.idle":"2026-02-17T17:13:05.463123Z","shell.execute_reply.started":"2026-02-17T17:13:05.450041Z","shell.execute_reply":"2026-02-17T17:13:05.462539Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\"\"\"Model setup\"\"\"\n\nmodel     = MobileNetTexture().to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n\nNUM_EPOCHS = 20\nbest_acc   = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.464043Z","iopub.execute_input":"2026-02-17T17:13:05.464312Z","iopub.status.idle":"2026-02-17T17:13:05.804674Z","shell.execute_reply.started":"2026-02-17T17:13:05.464279Z","shell.execute_reply":"2026-02-17T17:13:05.804030Z"}},"outputs":[{"name":"stderr","text":"Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\"\"\"Training\"\"\"\n\nfor epoch in range(NUM_EPOCHS):\n\n    model.train()\n    train_loss = 0\n    correct    = 0\n    total      = 0\n\n    for images, labels in tqdm(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images).squeeze()\n        loss    = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        preds       = (torch.sigmoid(outputs) > 0.5).float()\n        correct    += (preds == labels).sum().item()\n        total      += labels.size(0)\n\n    train_acc = 100 * correct / total\n\n    model.eval()\n    test_loss  = 0\n    correct    = 0\n    total      = 0\n    all_preds  = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images).squeeze()\n            loss    = criterion(outputs, labels)\n\n            test_loss += loss.item()\n\n            probs = torch.sigmoid(outputs)\n            preds = (probs > 0.5).float()\n\n            correct    += (preds == labels).sum().item()\n            total      += labels.size(0)\n\n            all_preds.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    test_acc = 100 * correct / total\n    auc      = roc_auc_score(all_labels, all_preds)\n\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    print(f\"Train Acc: {train_acc:.2f}%\")\n    print(f\"Test Acc:  {test_acc:.2f}% | AUC: {auc:.4f}\")\n\n    if test_acc > best_acc:\n        best_acc = test_acc\n        torch.save(model.state_dict(), \"/kaggle/working/best_model_texture-V2-resblock.pth\")\n        print(\"✓ Best model saved\")\n\n    scheduler.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:13:05.806840Z","iopub.execute_input":"2026-02-17T17:13:05.807146Z","iopub.status.idle":"2026-02-17T17:18:50.421540Z","shell.execute_reply.started":"2026-02-17T17:13:05.807120Z","shell.execute_reply":"2026-02-17T17:18:50.420639Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 44/44 [00:16<00:00,  2.74it/s]\n100%|██████████| 21/21 [00:02<00:00,  8.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/20\nTrain Acc: 80.22%\nTest Acc:  74.04% | AUC: 0.9123\n✓ Best model saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.96it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/20\nTrain Acc: 94.08%\nTest Acc:  80.18% | AUC: 0.9241\n✓ Best model saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.99it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/20\nTrain Acc: 95.72%\nTest Acc:  87.02% | AUC: 0.9486\n✓ Best model saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.92it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/20\nTrain Acc: 96.69%\nTest Acc:  89.25% | AUC: 0.9419\n✓ Best model saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.88it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/20\nTrain Acc: 97.29%\nTest Acc:  85.56% | AUC: 0.9493\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.90it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/20\nTrain Acc: 97.15%\nTest Acc:  88.25% | AUC: 0.9582\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.93it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/20\nTrain Acc: 98.75%\nTest Acc:  85.48% | AUC: 0.9490\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  3.00it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/20\nTrain Acc: 98.33%\nTest Acc:  87.48% | AUC: 0.9557\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.96it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/20\nTrain Acc: 98.86%\nTest Acc:  91.94% | AUC: 0.9539\n✓ Best model saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  3.02it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/20\nTrain Acc: 98.82%\nTest Acc:  88.17% | AUC: 0.9622\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.94it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/20\nTrain Acc: 98.90%\nTest Acc:  90.63% | AUC: 0.9667\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.95it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/20\nTrain Acc: 99.04%\nTest Acc:  90.25% | AUC: 0.9592\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.98it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/20\nTrain Acc: 98.75%\nTest Acc:  90.55% | AUC: 0.9576\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.98it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/20\nTrain Acc: 98.93%\nTest Acc:  90.94% | AUC: 0.9620\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.89it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/20\nTrain Acc: 99.04%\nTest Acc:  90.02% | AUC: 0.9595\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.90it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/20\nTrain Acc: 99.00%\nTest Acc:  90.09% | AUC: 0.9625\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.96it/s]\n100%|██████████| 21/21 [00:02<00:00, 10.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/20\nTrain Acc: 99.22%\nTest Acc:  90.40% | AUC: 0.9612\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.98it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/20\nTrain Acc: 99.50%\nTest Acc:  90.32% | AUC: 0.9635\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:15<00:00,  2.89it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/20\nTrain Acc: 99.43%\nTest Acc:  90.71% | AUC: 0.9640\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44/44 [00:14<00:00,  2.95it/s]\n100%|██████████| 21/21 [00:02<00:00,  9.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/20\nTrain Acc: 99.25%\nTest Acc:  91.17% | AUC: 0.9646\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ── ONNX Export ───────────────────────────────────────────────────────────────\n\n\"\"\"Rebuild and load best model\"\"\"\n\nmodel_export = MobileNetTexture().to(device)\nmodel_export.load_state_dict(torch.load(\n    \"/kaggle/working/best_model_texture-V2-resblock.pth\",\n    map_location=device\n))\nmodel_export.eval()\nprint(\"✓ Best model loaded for export\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:23:16.987124Z","iopub.execute_input":"2026-02-17T17:23:16.988049Z","iopub.status.idle":"2026-02-17T17:23:17.185286Z","shell.execute_reply.started":"2026-02-17T17:23:16.988004Z","shell.execute_reply":"2026-02-17T17:23:17.184701Z"}},"outputs":[{"name":"stderr","text":"Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n","output_type":"stream"},{"name":"stdout","text":"✓ Best model loaded for export\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Verify forward pass\ndummy_input = torch.randn(1, 3, 224, 224).to(device)\nwith torch.no_grad():\n    output = model_export(dummy_input)\nprint(\"Output shape:\", output.shape)\nprint(\"Raw output:  \", output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:23:28.627110Z","iopub.execute_input":"2026-02-17T17:23:28.627381Z","iopub.status.idle":"2026-02-17T17:23:28.815492Z","shell.execute_reply.started":"2026-02-17T17:23:28.627358Z","shell.execute_reply":"2026-02-17T17:23:28.814836Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([1, 1])\nRaw output:   tensor([[-36.0639]], device='cuda:0')\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\"\"\"Export to ONNX\"\"\"\n\nimport subprocess\nsubprocess.run([\"pip\", \"install\", \"onnxruntime\", \"onnxscript\", \"-q\"], check=True)\nimport onnxruntime as ort\n\nonnx_path = \"/kaggle/working/antispoof_v2_depthwise.onnx\"\n\ntorch.onnx.export(\n    model_export,\n    dummy_input,\n    onnx_path,\n    export_params=True,\n    opset_version=13,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={\n        'input':  {0: 'batch_size'},\n        'output': {0: 'batch_size'}\n    },\n    external_data=False\n)\nprint(f\"✓ ONNX model exported → {onnx_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:23:44.001820Z","iopub.execute_input":"2026-02-17T17:23:44.002551Z","iopub.status.idle":"2026-02-17T17:23:50.887343Z","shell.execute_reply.started":"2026-02-17T17:23:44.002515Z","shell.execute_reply":"2026-02-17T17:23:50.886703Z"}},"outputs":[{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 91.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 689.1/689.1 kB 45.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 159.3/159.3 kB 14.3 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_989/2552773357.py:9: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n  torch.onnx.export(\n","output_type":"stream"},{"name":"stdout","text":"✓ ONNX model exported → /kaggle/working/antispoof_v2_depthwise.onnx\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\"\"\"Verify ONNX output matches PyTorch\"\"\"\n\ndummy_input = torch.randn(1, 3, 224, 224).to(device)\n\nwith torch.no_grad():\n    torch_output = model_export(dummy_input).cpu().numpy()\n\nort_session = ort.InferenceSession(onnx_path)\nort_inputs  = {ort_session.get_inputs()[0].name: dummy_input.cpu().numpy()}\nort_output  = ort_session.run(None, ort_inputs)[0]\n\nprint(\"PyTorch output:\", torch_output)\nprint(\"ONNX output:   \", ort_output)\nprint(\"Difference:    \", abs(torch_output - ort_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T17:24:23.492260Z","iopub.execute_input":"2026-02-17T17:24:23.492854Z","iopub.status.idle":"2026-02-17T17:24:23.543632Z","shell.execute_reply.started":"2026-02-17T17:24:23.492824Z","shell.execute_reply":"2026-02-17T17:24:23.542936Z"}},"outputs":[{"name":"stdout","text":"PyTorch output: [[-61.599277]]\nONNX output:    [[-61.600605]]\nDifference:     [[0.00132751]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}