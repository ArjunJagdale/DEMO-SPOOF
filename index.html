<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Anti-Spoof Live</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<style>
  * {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }
  body {
    text-align: center;
    font-family: Arial;
    overflow-x: hidden;
  }
  h2 {
    font-size: 30px;
    margin: 16px 0 6px;
  }
  .instructions {
    font-size: 15px;
    color: #444;
    max-width: 360px;
    margin: 8px auto 16px;
    line-height: 1.4;
    padding: 0 12px;
  }
  .video-wrapper {
    position: relative;
    width: min(400px, calc(100vw - 24px));
    height: min(400px, calc(100vw - 24px));
    margin: 0 auto;
    overflow: hidden;
  }
  video {
    width: 100%;
    height: 100%;
    object-fit: cover;
    transform: scaleX(-1);
    display: block;
  }
  .video-wrapper::after {
    content: "";
    position: absolute;
    inset: 0;
    pointer-events: none;
    background:
      radial-gradient(
        ellipse 35% 55% at center,
        transparent 0%,
        transparent 90%,
        rgba(0, 0, 0, 0.80) 91%
      );
  }
</style>
</head>
<body>
<h2>Live Anti-Spoof</h2>
<p class="instructions">
  • Ensure your face is clearly visible in good lighting (avoid harsh reflections).<br>
  • Align and fit your face tightly within the oval.
</p>
<div class="video-wrapper">
  <video id="video" autoplay playsinline></video>
</div>
<canvas id="canvas" width="224" height="224" style="display:none;"></canvas>
<p id="result" style="margin-top:12px;">Loading model...</p>
<script>
let session;
let faceDetector;

// Oval region in 224x224 canvas space
const OVAL_CX = 112, OVAL_CY = 112;
const OVAL_RX = 224 * 0.35;   // 35% of width
const OVAL_RY = 224 * 0.55;   // 55% of height

// Min face size to be considered "close enough" (% of canvas)
const MIN_FACE_RATIO = 0.30;   // face width must be ≥ 30% of 224px

function faceInOval(face, canvasW, canvasH) {
  // face bbox comes from the video element size, scale to canvas size
  const scaleX = canvasW / face.boundingBox.width;  // rough scale — we use center point
  const fx = (face.boundingBox.x + face.boundingBox.width  / 2) / canvasW * 224;
  const fy = (face.boundingBox.y + face.boundingBox.height / 2) / canvasH * 224;
  const fw = face.boundingBox.width  / canvasW * 224;

  // Check if face center is inside oval
  const inOval = ((fx - OVAL_CX) ** 2 / OVAL_RX ** 2 +
                  (fy - OVAL_CY) ** 2 / OVAL_RY ** 2) <= 1;

  const closeEnough = fw >= MIN_FACE_RATIO * 224;

  return { inOval, closeEnough };
}

async function init() {
  // Load ONNX model
  session = await ort.InferenceSession.create("optimized_antispoof_cnn_int8.onnx");
  document.getElementById("result").innerText = "Model loaded. Starting camera...";

  // Init Face Detector if supported
  if ("FaceDetector" in window) {
    faceDetector = new FaceDetector({ fastMode: true });
  }

  startCamera();
}

async function startCamera() {
  const video = document.getElementById("video");
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "user" }
  });
  video.srcObject = stream;
  runLoop();
}

async function runLoop() {
  const video  = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const ctx    = canvas.getContext("2d");
  const result = document.getElementById("result");

  // Draw mirrored frame to canvas
  ctx.save();
  ctx.scale(-1, 1);
  ctx.drawImage(video, -224, 0, 224, 224);
  ctx.restore();

  // ── Face detection gating ──────────────────────────
  if (faceDetector) {
    try {
      const faces = await faceDetector.detect(canvas);

      if (faces.length === 0) {
        result.innerText = "No face detected";
        requestAnimationFrame(runLoop);
        return;
      }

      const { inOval, closeEnough } = faceInOval(faces[0], canvas.width, canvas.height);

      if (!closeEnough) {
        result.innerText = "Move closer ↑";
        requestAnimationFrame(runLoop);
        return;
      }

      if (!inOval) {
        result.innerText = "Align face to oval";
        requestAnimationFrame(runLoop);
        return;
      }

    } catch (e) {
      // FaceDetector failed silently — just proceed with inference
    }
  }
  // If FaceDetector not supported, skip gating and always infer

  // ── Preprocess ────────────────────────────────────
  const imageData = ctx.getImageData(0, 0, 224, 224).data;
  const floatData = new Float32Array(1 * 3 * 224 * 224);
  const mean = [0.485, 0.456, 0.406];
  const std  = [0.229, 0.224, 0.225];
  for (let i = 0; i < 224 * 224; i++) {
    const r = imageData[i * 4]     / 255;
    const g = imageData[i * 4 + 1] / 255;
    const b = imageData[i * 4 + 2] / 255;
    floatData[i]             = (r - mean[0]) / std[0];
    floatData[i + 224*224]   = (g - mean[1]) / std[1];
    floatData[i + 2*224*224] = (b - mean[2]) / std[2];
  }

  // ── Inference ─────────────────────────────────────
  const tensor  = new ort.Tensor("float32", floatData, [1, 3, 224, 224]);
  const results = await session.run({ input: tensor });
  const logit   = results.output.data[0];
  const prob    = 1 / (1 + Math.exp(-logit));

  result.innerText = prob > 0.5
    ? `REAL (${prob.toFixed(3)})`
    : `ATTACK (${prob.toFixed(3)})`;

  requestAnimationFrame(runLoop);
}

init();
</script>
</body>
</html>
