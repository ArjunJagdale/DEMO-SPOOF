<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Anti-Spoof Live</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script type="module">
    import { FaceDetector, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/vision_bundle.js";

    let session;
    let faceDetector;

    const OVAL_CX = 112, OVAL_CY = 112;
    const OVAL_RX = 224 * 0.35;
    const OVAL_RY = 224 * 0.55;
    const MIN_FACE_RATIO = 0.30;

    async function init() {
      // Load ONNX model
      session = await ort.InferenceSession.create("optimized_antispoof_cnn_int8.onnx");
      document.getElementById("result").innerText = "Loading face detector...";

      // Load MediaPipe Face Detector
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );
      faceDetector = await FaceDetector.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite",
          delegate: "GPU"
        },
        runningMode: "VIDEO"
      });

      document.getElementById("result").innerText = "Ready. Starting camera...";
      startCamera();
    }

    async function startCamera() {
      const video = document.getElementById("video");
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: "user" }
      });
      video.srcObject = stream;
      video.onloadedmetadata = () => runLoop();
    }

    async function runLoop() {
      const video  = document.getElementById("video");
      const canvas = document.getElementById("canvas");
      const ctx    = canvas.getContext("2d");
      const result = document.getElementById("result");

      // Draw mirrored frame
      ctx.save();
      ctx.scale(-1, 1);
      ctx.drawImage(video, -224, 0, 224, 224);
      ctx.restore();

      // ── MediaPipe face detection ───────────────────
      const detections = faceDetector.detectForVideo(canvas, performance.now()).detections;

      if (detections.length === 0) {
        result.innerText = "No face detected";
        requestAnimationFrame(runLoop);
        return;
      }

      // bbox is normalized 0-1, scale to 224
      const box = detections[0].boundingBox;
      const fx  = (box.originX + box.width  / 2) * 224;
      const fy  = (box.originY + box.height / 2) * 224;
      const fw  =  box.width * 224;

      const inOval = ((fx - OVAL_CX) ** 2 / OVAL_RX ** 2 +
                      (fy - OVAL_CY) ** 2 / OVAL_RY ** 2) <= 1;
      const closeEnough = fw >= MIN_FACE_RATIO * 224;

      if (!closeEnough) {
        result.innerText = "Move closer ↑";
        requestAnimationFrame(runLoop);
        return;
      }
      if (!inOval) {
        result.innerText = "Align face to oval";
        requestAnimationFrame(runLoop);
        return;
      }

      // ── Preprocess ────────────────────────────────
      const imageData = ctx.getImageData(0, 0, 224, 224).data;
      const floatData = new Float32Array(1 * 3 * 224 * 224);
      const mean = [0.485, 0.456, 0.406];
      const std  = [0.229, 0.224, 0.225];
      for (let i = 0; i < 224 * 224; i++) {
        const r = imageData[i * 4]     / 255;
        const g = imageData[i * 4 + 1] / 255;
        const b = imageData[i * 4 + 2] / 255;
        floatData[i]             = (r - mean[0]) / std[0];
        floatData[i + 224*224]   = (g - mean[1]) / std[1];
        floatData[i + 2*224*224] = (b - mean[2]) / std[2];
      }

      // ── Inference ─────────────────────────────────
      const tensor  = new ort.Tensor("float32", floatData, [1, 3, 224, 224]);
      const outputs = await session.run({ input: tensor });
      const logit   = outputs.output.data[0];
      const prob    = 1 / (1 + Math.exp(-logit));

      result.innerText = prob > 0.5
        ? `REAL (${prob.toFixed(3)})`
        : `ATTACK (${prob.toFixed(3)})`;

      requestAnimationFrame(runLoop);
    }

    init();
  </script>
<style>
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body { text-align: center; font-family: Arial; overflow-x: hidden; }
  h2 { font-size: 30px; margin: 16px 0 6px; }
  .instructions {
    font-size: 15px; color: #444; max-width: 360px;
    margin: 8px auto 16px; line-height: 1.4; padding: 0 12px;
  }
  .video-wrapper {
    position: relative;
    width: min(400px, calc(100vw - 24px));
    height: min(400px, calc(100vw - 24px));
    margin: 0 auto; overflow: hidden;
  }
  video { width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); display: block; }
  .video-wrapper::after {
    content: ""; position: absolute; inset: 0; pointer-events: none;
    background: radial-gradient(ellipse 35% 55% at center,
      transparent 0%, transparent 90%, rgba(0,0,0,0.80) 91%);
  }
</style>
</head>
<body>
<h2>Live Anti-Spoof</h2>
<p class="instructions">
  • Ensure your face is clearly visible in good lighting (avoid harsh reflections).<br>
  • Align and fit your face tightly within the oval.
</p>
<div class="video-wrapper">
  <video id="video" autoplay playsinline></video>
</div>
<canvas id="canvas" width="224" height="224" style="display:none;"></canvas>
<p id="result" style="margin-top:12px;">Loading model...</p>
</body>
</html>
